{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1111419_1DConv_reg.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMbrkWPE4+hRHWihcUK9+VE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"3BI7roHL7UG0","colab_type":"text"},"source":["**Mounting the drive**"]},{"cell_type":"code","metadata":{"id":"anUqgF677KN1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"outputId":"d46388fa-871e-4da4-c8a4-ee33ee7949cc","executionInfo":{"status":"ok","timestamp":1581597342173,"user_tz":300,"elapsed":570,"user":{"displayName":"Bhavik Ray","photoUrl":"","userId":"11882956498723180629"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":294,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4mKBUxYR7p4d","colab_type":"text"},"source":["**Importing important libraries**"]},{"cell_type":"code","metadata":{"id":"MoAN8vRq70Lm","colab_type":"code","colab":{}},"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import time"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qEMo48AvFfEy","colab_type":"text"},"source":["**Reading and removing incomplete data from dataset Housing.csv**"]},{"cell_type":"code","metadata":{"id":"4V-kl1FGFeN9","colab_type":"code","colab":{}},"source":["dataset = pd.read_csv('/content/drive/My Drive/housing.csv')\n","dataset = dataset.dropna()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r3gVhAFzGF8C","colab_type":"text"},"source":["**Printing the first ten records from dataset**"]},{"cell_type":"code","metadata":{"id":"b6E5QYI7GU0G","colab_type":"code","colab":{}},"source":["print(\"Here are the first ten rows of dataset:\")\n","dataset.head(10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b2kmqbs7-0YD","colab_type":"text"},"source":["**Plotting the features in a single graph**"]},{"cell_type":"code","metadata":{"id":"sIZS6kWQ9_6x","colab_type":"code","colab":{}},"source":["plt.figure()\n","dataset.plot(subplots=True,figsize=(10,10))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XU5zYo2LDR9l","colab_type":"text"},"source":["**Processing the dataset**"]},{"cell_type":"code","metadata":{"id":"cLp2wiv9Kepl","colab_type":"code","colab":{}},"source":["Y = dataset[\"median_house_value\"]\n","#From column longitude to median income will predict the value of Y\n","X = dataset.loc[:,'longitude':'median_income']\n","\n","#Here I have split the training and testing dataset ratio is set to 70:30 and set the random state\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state = 2003)\n","\n","#To use Pytorch library we convert dataset to numpy\n","x_train_np = x_train.to_numpy()\n","y_train_np = y_train.to_numpy()\n","\n","x_test_np = x_test.to_numpy()\n","y_test_np = y_test.to_numpy()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K0r1ULQuD6nJ","colab_type":"text"},"source":["**Importing libraries for creating model**"]},{"cell_type":"code","metadata":{"id":"PYcgMwEIGrME","colab_type":"code","colab":{}},"source":["import torch\n","from torch.nn import Conv1d   #Importing 1D convolution layer\n","from torch.nn import Dropout  #Importing dropout layer to handling the overfitting issues in the code\n","from torch.nn import MaxPool1d  #Importing the max pooling layer\n","from torch.nn import Flatten  #Importing the flatten layer\n","from torch.nn import Linear   #Importing the linear layer\n","from torch.nn.functional import relu  #Importing the relu activation function\n","from torch.utils.data import DataLoader, TensorDataset"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WSDveBnSGs24","colab_type":"text"},"source":["**Creating the neural network model**"]},{"cell_type":"code","metadata":{"id":"wBoFq19hIYgI","colab_type":"code","colab":{}},"source":["class CnnRegressor(torch.nn.Module):\n","  def __init__(self, batch_size, inputs, outputs):\n","    super(CnnRegressor, self).__init__()\n","    self.batch_size = batch_size\n","    self.inputs = inputs\n","    self.outputs = outputs\n","    self.dropout = Dropout(0.8)\n","    self.input_layer = Conv1d(inputs, batch_size, 1)\n","    self.max_pooling_layer = MaxPool1d(1)\n","    self.conv_layer = Conv1d(batch_size, 256, 1)\n","    self.flatten_layer = Flatten()\n","    self.linear_layer = Linear(256, 64)\n","    self.output_layer = Linear(64, outputs)\n","  \n","  def feed(self, input):\n","    input = input.reshape((self.batch_size, self.inputs, 1))\n","    output = relu(self.input_layer(input))\n","    output = self.max_pooling_layer(output)\n","    output = relu(self.conv_layer(output))\n","    output = self.flatten_layer(output)\n","    output = self.linear_layer(output)\n","    output = self.dropout(output)\n","    output = self.output_layer(output)\n","    return output"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"af3Vp_KoSnBR","colab_type":"text"},"source":["**Importing Libraries and R2Score**"]},{"cell_type":"code","metadata":{"id":"P_fAZb22QFn7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"b5abb740-d461-42b1-e212-660b6ee81d1b","executionInfo":{"status":"ok","timestamp":1581597348748,"user_tz":300,"elapsed":7083,"user":{"displayName":"Bhavik Ray","photoUrl":"","userId":"11882956498723180629"}}},"source":["from torch.optim import SGD\n","from torch.optim import Adadelta\n","from torch.optim import Adam\n","from torch.optim import Adagrad\n","from torch.nn import L1Loss\n","!pip install pytorch-ignite\n","from ignite.contrib.metrics.regression.r2_score import R2Score"],"execution_count":302,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pytorch-ignite in /usr/local/lib/python3.6/dist-packages (0.3.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pytorch-ignite) (1.4.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uvmOdnqnDph0","colab_type":"text"},"source":["**Defining the model**"]},{"cell_type":"code","metadata":{"id":"rIC_RGn-QSIh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":163},"outputId":"5ad79314-37d9-4f9d-f10d-5eded9c8d7a7","executionInfo":{"status":"ok","timestamp":1581597348750,"user_tz":300,"elapsed":7080,"user":{"displayName":"Bhavik Ray","photoUrl":"","userId":"11882956498723180629"}}},"source":["batch_size = 64\n","model = CnnRegressor(batch_size, X.shape[1], 1)\n","model.cuda()"],"execution_count":303,"outputs":[{"output_type":"execute_result","data":{"text/plain":["CnnRegressor(\n","  (dropout): Dropout(p=0.8, inplace=False)\n","  (input_layer): Conv1d(8, 64, kernel_size=(1,), stride=(1,))\n","  (max_pooling_layer): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n","  (conv_layer): Conv1d(64, 256, kernel_size=(1,), stride=(1,))\n","  (flatten_layer): Flatten()\n","  (linear_layer): Linear(in_features=256, out_features=64, bias=True)\n","  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":303}]},{"cell_type":"code","metadata":{"id":"ngAAoOJGQfNr","colab_type":"code","colab":{}},"source":["def model_loss(model, dataset, train = False, optimizer = None):\n","  performance = L1Loss()\n","  score_metric = R2Score()\n","\n","  avg_loss = 0\n","  avg_score = 0\n","  count = 0\n","  for input, output in iter(dataset):\n","    \n","    #Getting the model's preidctions for training dataset\n","    predictions = model.feed(input)\n","\n","    #Getting the model loss\n","    loss = performance(predictions, output)\n","    \n","    #Getting the model's R2 score\n","    score_metric.update([predictions, output])\n","    score = score_metric.compute()\n","\n","    if(train):\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","      \n","    avg_loss += loss.item()\n","    avg_score += score\n","    count += 1\n","  return avg_loss / count, avg_score / count"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6LMMH6QhBvOC","colab_type":"text"},"source":["**Training the model for dataset**"]},{"cell_type":"code","metadata":{"id":"3Ux0OscWQhPb","colab_type":"code","colab":{}},"source":["#Defining the number of epochs to be trained\n","epochs = 500\n","\n","#Defining the optimizer \n","#optimizer = SGD(model.parameters(), lr=1e-5)\n","#optimizer= Adadelta(model.parameters(), lr=0.1, rho=0.4, eps=1e-06, weight_decay=0)\n","#optimizer = Adam(model.parameters())\n","optimizer = Adagrad(model.parameters(), lr=0.3, eps=1e-10,weight_decay=0, initial_accumulator_value=0)\n","#Converting testing set into torch variable as floats\n","inputs = torch.from_numpy(x_train_np).cuda().float()\n","outputs = torch.from_numpy(y_train_np.reshape(y_train_np.shape[0], 1)).cuda().float()\n","\n","#Creating DataLoader instance\n","tensor = TensorDataset(inputs, outputs)\n","loader = DataLoader(tensor, batch_size, shuffle=True, drop_last=True)\n","\n","for epoch in range(epochs):\n","  avg_loss, avg_r2_score = model_loss(model, loader, train=True, optimizer=optimizer)\n","\n","  print(\"Epoch \" + str(epoch + 1) + \":\\n\\tLoss = \" + str(avg_loss) + \"\\n\\tR^2 Score = \" + str(avg_r2_score))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8dz9l-X3B5Dc","colab_type":"text"},"source":["**Testing the model and Showing the final results with inference time for testing** "]},{"cell_type":"code","metadata":{"id":"yzZdBKc6RSMG","colab_type":"code","colab":{}},"source":["inputs = torch.from_numpy(x_test_np).cuda().float()\n","outputs = torch.from_numpy(y_test_np.reshape(y_test_np.shape[0],1)).cuda().float()\n","\n","#Inference time for testing\n","start_time = time.time()\n","\n","tensor = TensorDataset(inputs,outputs)\n","loader = DataLoader(tensor,batch_size,shuffle=True,drop_last=True)\n","\n","avg_loss, avg_r2_score = model_loss(model, loader)\n","print(\"The model's L1lossis:\"+str(avg_loss))\n","print(\"The model's R^2scoreis:\"+str(avg_r2_score))\n","print(\"--- %s seconds ---\" % (time.time() - start_time))"],"execution_count":0,"outputs":[]}]}